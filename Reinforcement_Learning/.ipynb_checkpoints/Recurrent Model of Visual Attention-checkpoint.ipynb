{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recurrent Models of Visual Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 의사결정 RL\n",
    "* 강은숙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* (1) Overview\n",
    "* (2) Motivation\n",
    "* (3) Recurrent Attention Model (RAM)\n",
    "    * (3.1) Concept\n",
    "    * (3.2) Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 논문은 Google DeepMind가 NIPS 2014에서 발표한 Recurrent Neural Network와 Reinforcement Learning을 결합한 논문 입니다. \n",
    "\n",
    "예전에 한번 살펴본 Playing Atari With Deep Reinforcement Learning 에서는 전통적인 RL 문제인 '게임'을 풀기 위하여 CNN으로 action-value function을 모델링하고 value iteration을 대체하는 새로운 action-value function learning 모델과 알고리즘을 제안했다면, \n",
    "\n",
    "이 논문은 기존 RL 문제라기보다는 오히려 좀 더 클래식한 classification 문제라고 할 수 있는 image recognition 문제에 RNN 구조와 RL구조를 결합하여 reward maximazation optimization problem을 푸는 모델과 알고리즘을 제안 합니다.\n",
    "\n",
    "### RNN (Recurrent Neural Network) : 순환 신경망\n",
    "\n",
    "사람은 생각을 매 초 처음부터 하지 않습니다. 우리가 항상 어떤 글을 읽을 떄도 각 단어를 이전 단어에 기반을 두고 이해합니다. \n",
    "\n",
    "우리는 기존의 것을 모두 버리고 처음부터 생각을 하지 않습니다. 우리의 생각에는 지속성이 있습니다. \n",
    "\n",
    "순환 신경망은 이 주제를 다룹니다. 순환 신경망은 그 내부에 루프를 가진 네트워크로서 그 루프는 정보가 지속되는 것을 돕습니다. \n",
    "\n",
    "<img src = \"./rnn_1.png\">\n",
    "\n",
    "위 다이어그램에서 신경망 A는 입력값 xt를 보고 값 ht를 출력합니다. \n",
    "\n",
    "조금더 살펴보면 루프들은 보통신경망과 다르지 않습니다. \n",
    "\n",
    "순환 신경망은 동일한 네트워크 여러개를 복사한 것으로도 생각할 수 있습니다. \n",
    "\n",
    "각 네트워크는 자신의 후임자에게 메시지를 전달하고, 그 루프가 펼쳐지면 아래와 같은 모양과 같습니다. \n",
    "\n",
    "<img src = \"./rnn_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Recurrent Attention Model (RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.1) Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.2) Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
